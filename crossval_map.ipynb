{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55b40de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "import torch\n",
    "import os\n",
    "# Basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Monitoring\n",
    "from tqdm.notebook import tqdm\n",
    "# IO\n",
    "from os.path import join, exists, basename, dirname, splitext, expanduser\n",
    "from glob import glob\n",
    "# Parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "from PIL import Image\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "from supervision.metrics import MeanAveragePrecision\n",
    "\n",
    "\n",
    "from supervision.metrics.core import Metric, MetricTarget\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "245dcf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü© Fold 0 Evaluation\n",
      "Loaded 31 test samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79295aa2eee46b5aa4f82dda93ce310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise mAP: [0, 0.30164225213730167, 0.3684006558895774]\n",
      "\n",
      "üü© Fold 1 Evaluation\n",
      "Loaded 31 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b165cb7030451ab0eee447aa84bcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise mAP: [0, 0.12036775106082037, 0.20713032841745715]\n",
      "\n",
      "üü© Fold 2 Evaluation\n",
      "Loaded 31 test samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a90b036d8744768bbcd15174f10c6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise mAP: [0, 0.1995865520617996, 0.41495652840243014]\n",
      "\n",
      "üü© Fold 3 Evaluation\n",
      "Loaded 31 test samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea9426f28ee4dd192224695c9235013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise mAP: [0, 0.17232800203097234, 0.2218988107601969]\n",
      "‚úÖ Results saved with grid formatting:\n",
      "- Text: summary.txt\n",
      "- CSV : wb_small_airshed_crossval_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>üìç Cross-Validation Results ‚Äî WB SMALL AIRSHED</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Set CUDA device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# Parameters\n",
    "region = \"wb_small_airshed\"  # First region (change this for the second region)\n",
    "task = \"obb\"\n",
    "satellite_type = \"sentinel\"\n",
    "class_names = [\"CFCBK\", \"FCBK\", \"Zigzag\"]\n",
    "num_classes = len(class_names)\n",
    "CLASSES = class_names  # For sv.ConfusionMatrix\n",
    "\n",
    "# Define paths\n",
    "base_fold_path = \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/brick_kilns_neurips_2025/runs/obb\"\n",
    "crossval_base_path = f\"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/crossval/{region}_{task}_labels_{satellite_type}\"\n",
    "model_suffix = f\"{region}_obb_labels_sentinel_model_yolov8l-obb_epochs_100_{{}}_128/weights/best.pt\"\n",
    "\n",
    "# Create a temporary YAML file\n",
    "data_yml_save_path = mkdtemp()\n",
    "data_yml_path = os.path.join(data_yml_save_path, \"data.yml\")\n",
    "with open(data_yml_path, \"w\") as f:\n",
    "    f.write(f\"\"\"train: dummy\n",
    "val: dummy\n",
    "nc: {num_classes}\n",
    "names: {class_names}\n",
    "\"\"\")\n",
    "\n",
    "# Initialize results DataFrame\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'Fold', 'IoU', 'Precision', 'Recall', 'F1 score', \n",
    "    'TP', 'FP', 'FN', 'Kiln instances', 'mAP_cfcbk', 'mAP_fcbk', 'mAP_zigzag'\n",
    "])\n",
    "\n",
    "# Evaluate each fold\n",
    "for fold in range(4):\n",
    "    print(f\"\\nüü© Fold {fold} Evaluation\")\n",
    "\n",
    "    # Paths\n",
    "    model_path = os.path.join(base_fold_path, model_suffix.format(fold))\n",
    "    test_image_dir = os.path.join(crossval_base_path, str(fold), \"test/images\")\n",
    "    test_label_dir = os.path.join(crossval_base_path, str(fold), \"test/labels\")\n",
    "\n",
    "    # Load dataset\n",
    "    sv_dataset = sv.DetectionDataset.from_yolo(test_image_dir, test_label_dir, data_yml_path)\n",
    "    print(f\"Loaded {len(sv_dataset)} test samples\")\n",
    "\n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    targets, predictions = [], []\n",
    "\n",
    "    # Inference loop\n",
    "    for name, _, gt_detection in tqdm(sv_dataset):\n",
    "        result = model(\n",
    "            name,\n",
    "            imgsz=128,\n",
    "            iou=0.33,\n",
    "            conf=0.25,\n",
    "            max_det=300,\n",
    "            verbose=False\n",
    "        )[0]\n",
    "        prediction = sv.Detections.from_ultralytics(result)\n",
    "        predictions.append(prediction)\n",
    "        targets.append(gt_detection)\n",
    "\n",
    "    # mAP Calculation\n",
    "    mAP_metric = MeanAveragePrecision(class_agnostic=False)\n",
    "    mAP_result = mAP_metric.update(predictions, targets).compute()\n",
    "    class_wise_mAP = [0] * num_classes\n",
    "    for cls, mAP in zip(mAP_result.matched_classes.tolist(), mAP_result.ap_per_class[:, 0].tolist()):\n",
    "        class_wise_mAP[cls] = mAP\n",
    "    print(f\"Class-wise mAP: {class_wise_mAP}\")\n",
    "\n",
    "    # Confusion Matrix and Metrics at IoU=0.5\n",
    "    iou_threshold = 0.33\n",
    "    cm = sv.ConfusionMatrix.from_detections(\n",
    "        predictions=predictions,\n",
    "        targets=targets,\n",
    "        classes=CLASSES,\n",
    "        conf_threshold=0.25,\n",
    "        iou_threshold=iou_threshold\n",
    "    ).matrix\n",
    "\n",
    "    # True Positives\n",
    "    tp = sum(cm[i][i] for i in range(num_classes))\n",
    "\n",
    "    # Predicted Positives (Columns sum)\n",
    "    predicted_positives = cm.sum(axis=0).sum()\n",
    "\n",
    "    # Actual Positives (Rows sum)\n",
    "    actual_positives = cm.sum(axis=1).sum()\n",
    "\n",
    "    # Precision, Recall, F1 Score\n",
    "    precision = tp / (predicted_positives + 1e-9)\n",
    "    recall = tp / (actual_positives + 1e-9)\n",
    "    f1_score = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "\n",
    "    fp = predicted_positives - tp\n",
    "    fn = actual_positives - tp\n",
    "\n",
    "    # Append results\n",
    "    # Append results\n",
    "    results_df = pd.concat([\n",
    "        results_df,\n",
    "        pd.DataFrame([{\n",
    "            'Fold': fold,\n",
    "            'IoU': round(iou_threshold, 2),\n",
    "            'Precision': round(precision, 2),\n",
    "            'Recall': round(recall, 2),\n",
    "            'F1 score': round(f1_score, 2),\n",
    "            'TP': tp,\n",
    "            'FP': fp,\n",
    "            'FN': fn,\n",
    "            'Kiln instances': actual_positives,\n",
    "            'mAP_cfcbk': round(class_wise_mAP[0], 2),\n",
    "            'mAP_fcbk': round(class_wise_mAP[1], 2),\n",
    "            'mAP_zigzag': round(class_wise_mAP[2], 2)\n",
    "        }])\n",
    "    ], ignore_index=True)\n",
    "\n",
    "# Compute mean and variance for all numeric columns (excluding 'Fold' and 'IoU')\n",
    "metrics_to_summarize = ['Precision', 'Recall', 'F1 score', 'TP', 'FP', 'FN', 'Kiln instances', 'mAP_cfcbk', 'mAP_fcbk', 'mAP_zigzag']\n",
    "mean_values = results_df[metrics_to_summarize].mean()\n",
    "std_values = results_df[metrics_to_summarize].std()\n",
    "\n",
    "# Format as \"mean ¬± std\"\n",
    "summary_row = {\n",
    "    'Fold': 'mean ¬± std',\n",
    "    'IoU': '-'\n",
    "}\n",
    "for metric in metrics_to_summarize:\n",
    "    summary_row[metric] = f\"{mean_values[metric]:.2f} ¬± {std_values[metric]:.2f}\"\n",
    "\n",
    "# Append the formatted summary row\n",
    "results_df = pd.concat([\n",
    "    results_df,\n",
    "    pd.DataFrame([summary_row])\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = f\"{region}_crossval_results.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Create formatted table with grid lines\n",
    "region_title = f\"üìç Cross-Validation Results ‚Äî {region.replace('_', ' ').upper()}\"\n",
    "table_string = tabulate(results_df, headers='keys', tablefmt='grid', showindex=False)\n",
    "\n",
    "# Save to TXT (append mode)\n",
    "txt_path = \"summary.txt\"  # Change this to your desired path\n",
    "with open(txt_path, \"a\") as f:  # Open in append mode\n",
    "    f.write(f\"\\n{region_title.center(120)}\\n\\n\")\n",
    "    f.write(table_string)\n",
    "\n",
    "print(f\"‚úÖ Results saved with grid formatting:\\n- Text: {txt_path}\\n- CSV : {csv_path}\")\n",
    "# Display the table in Jupyter Notebook\n",
    "display(HTML(f\"<h3>{region_title}</h3>\"))\n",
    "# Display the table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71dfc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
