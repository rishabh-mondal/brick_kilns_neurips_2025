{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942cb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "080e5ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'common' (buffered, touching/overlapping): 29972\n",
      "Count of 'unique' (buffered): 67676\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import pandas as pd\n",
    "def get_latlon_corners(center_lat, center_lon, points, img_size=128, gsd=10):\n",
    "    latlons = []\n",
    "    for x, y in points:\n",
    "        dx = x - img_size / 2\n",
    "        dy = y - img_size / 2\n",
    "        dx_m = dx * gsd\n",
    "        dy_m = dy * gsd\n",
    "        dlat = dy_m / 111320\n",
    "        dlon = dx_m / (111320 * np.cos(np.deg2rad(center_lat)))\n",
    "        lat = center_lat + dlat\n",
    "        lon = center_lon + dlon\n",
    "        latlons.append((lon, lat))\n",
    "    return latlons\n",
    "\n",
    "def folder_to_gdf(folder, split):\n",
    "    data = []\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname.endswith('.txt'):\n",
    "            image_name = fname.replace('.txt', '.png')\n",
    "            lat_str, lon_str = image_name.replace('.png', '').split('_')\n",
    "            center_lat = float(lat_str)\n",
    "            center_lon = float(lon_str)\n",
    "            label_path = os.path.join(folder, fname)\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 9:\n",
    "                        continue\n",
    "                    class_id = int(parts[0])\n",
    "                    coords = list(map(float, parts[1:]))\n",
    "                    points = np.array(coords).reshape(4,2) * 128\n",
    "                    latlons = get_latlon_corners(center_lat, center_lon, points)\n",
    "                    poly = Polygon(latlons)\n",
    "                    data.append({\n",
    "                        'image_name': image_name,\n",
    "                        'class': class_id,\n",
    "                        'geometry': poly,\n",
    "                        'split': split\n",
    "                    })\n",
    "    return gpd.GeoDataFrame(data, crs=\"EPSG:4326\")\n",
    "\n",
    "folders = {\n",
    "    \"train\": \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/train/yolo_obb_labels\",\n",
    "    \"val\":   \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/val/yolo_obb_labels\",\n",
    "    \"test\":  \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/stratified_split/clean_test/yolo_obb_labels\"\n",
    "}\n",
    "\n",
    "gdfs = [folder_to_gdf(folders[split], split) for split in [\"train\", \"val\", \"test\"]]\n",
    "gdf_all = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "# Make a buffer of 0.5 meter around each geometry (requires projected CRS, not EPSG:4326)\n",
    "# Project to UTM (auto) for metric units\n",
    "gdf_proj = gdf_all.to_crs(gdf_all.estimate_utm_crs())\n",
    "gdf_proj['buffered'] = gdf_proj.geometry.buffer(16.033)  # 0.5 meter buffer\n",
    "\n",
    "# Use buffered geometries for overlap analysis\n",
    "sindex = gdf_proj['buffered'].sindex\n",
    "common_idx = set()\n",
    "\n",
    "for idx, geom in enumerate(gdf_proj['buffered']):\n",
    "    possible_matches = list(sindex.intersection(geom.bounds))\n",
    "    for other_idx in possible_matches:\n",
    "        if other_idx == idx:\n",
    "            continue\n",
    "        other_geom = gdf_proj['buffered'].iloc[other_idx]\n",
    "        if geom.intersects(other_geom):\n",
    "            intersection_area = geom.intersection(other_geom).area\n",
    "            min_area = min(geom.area, other_geom.area)\n",
    "            if min_area > 0 and intersection_area / min_area > 1e-6:\n",
    "                common_idx.add(idx)\n",
    "                common_idx.add(other_idx)\n",
    "            elif intersection_area == 0 and geom.touches(other_geom):\n",
    "                common_idx.add(idx)\n",
    "                common_idx.add(other_idx)\n",
    "\n",
    "gdf_all['grouped'] = 'unique'\n",
    "gdf_all.loc[list(common_idx), 'grouped'] = 'common'\n",
    "print(\"Count of 'common' (buffered, touching/overlapping):\", (gdf_all['grouped'] == 'common').sum())\n",
    "print(\"Count of 'unique' (buffered):\", (gdf_all['grouped'] == 'unique').sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df44f7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>class</th>\n",
       "      <th>geometry</th>\n",
       "      <th>split</th>\n",
       "      <th>grouped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.6379_74.0288.png</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((74.03297 29.64058, 74.033 29.6395, 7...</td>\n",
       "      <td>train</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.6379_74.0288.png</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((74.03439 29.63623, 74.03505 29.63621...</td>\n",
       "      <td>train</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.8540_85.0146.png</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((85.01144 24.85198, 85.0117 24.85117,...</td>\n",
       "      <td>train</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.7261_89.5030.png</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((89.49756 23.73071, 89.49789 23.73065...</td>\n",
       "      <td>train</td>\n",
       "      <td>common</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31.4750_73.3851.png</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((73.38709 31.47457, 73.38792 31.47408...</td>\n",
       "      <td>train</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97643</th>\n",
       "      <td>25.3086_69.1853.png</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((69.18505 25.30658, 69.18576 25.30601...</td>\n",
       "      <td>test</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97644</th>\n",
       "      <td>29.6643_77.2661.png</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((77.27013 29.66882, 77.27055 29.6685,...</td>\n",
       "      <td>test</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97645</th>\n",
       "      <td>25.9761_88.6232.png</td>\n",
       "      <td>2</td>\n",
       "      <td>POLYGON ((88.62826 25.97504, 88.62855 25.97441...</td>\n",
       "      <td>test</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97646</th>\n",
       "      <td>27.4379_80.3073.png</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((80.31166 27.43625, 80.31214 27.4354,...</td>\n",
       "      <td>test</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97647</th>\n",
       "      <td>27.7556_76.1077.png</td>\n",
       "      <td>1</td>\n",
       "      <td>POLYGON ((76.10835 27.75302, 76.10855 27.75261...</td>\n",
       "      <td>test</td>\n",
       "      <td>unique</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97648 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                image_name  class  \\\n",
       "0      29.6379_74.0288.png      1   \n",
       "1      29.6379_74.0288.png      1   \n",
       "2      24.8540_85.0146.png      1   \n",
       "3      23.7261_89.5030.png      2   \n",
       "4      31.4750_73.3851.png      1   \n",
       "...                    ...    ...   \n",
       "97643  25.3086_69.1853.png      1   \n",
       "97644  29.6643_77.2661.png      2   \n",
       "97645  25.9761_88.6232.png      2   \n",
       "97646  27.4379_80.3073.png      1   \n",
       "97647  27.7556_76.1077.png      1   \n",
       "\n",
       "                                                geometry  split grouped  \n",
       "0      POLYGON ((74.03297 29.64058, 74.033 29.6395, 7...  train  unique  \n",
       "1      POLYGON ((74.03439 29.63623, 74.03505 29.63621...  train  common  \n",
       "2      POLYGON ((85.01144 24.85198, 85.0117 24.85117,...  train  unique  \n",
       "3      POLYGON ((89.49756 23.73071, 89.49789 23.73065...  train  common  \n",
       "4      POLYGON ((73.38709 31.47457, 73.38792 31.47408...  train  unique  \n",
       "...                                                  ...    ...     ...  \n",
       "97643  POLYGON ((69.18505 25.30658, 69.18576 25.30601...   test  unique  \n",
       "97644  POLYGON ((77.27013 29.66882, 77.27055 29.6685,...   test  unique  \n",
       "97645  POLYGON ((88.62826 25.97504, 88.62855 25.97441...   test  unique  \n",
       "97646  POLYGON ((80.31166 27.43625, 80.31214 27.4354,...   test  unique  \n",
       "97647  POLYGON ((76.10835 27.75302, 76.10855 27.75261...   test  unique  \n",
       "\n",
       "[97648 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab379dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique kilns in train split: 42545\n"
     ]
    }
   ],
   "source": [
    "# Count of unique kilns in test split\n",
    "count_unique_test = ((gdf_all['split'] == 'train') & (gdf_all['grouped'] == 'unique')).sum()\n",
    "print(f\"Unique kilns in train split: {count_unique_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47bc28a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique kilns in val split: 13961\n"
     ]
    }
   ],
   "source": [
    "# Count of unique kilns in test split\n",
    "count_unique_test = ((gdf_all['split'] == 'val') & (gdf_all['grouped'] == 'unique')).sum()\n",
    "print(f\"Unique kilns in val split: {count_unique_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e8afd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique kilns in test split: 11170\n"
     ]
    }
   ],
   "source": [
    "# Count of unique kilns in test split\n",
    "count_unique_test = ((gdf_all['split'] == 'test') & (gdf_all['grouped'] == 'unique')).sum()\n",
    "print(f\"Unique kilns in test split: {count_unique_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "484bb721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test instances: 21100\n",
      "Clean test instances (no overlap with train/val): 14418\n",
      "class\n",
      "1    7859\n",
      "2    6165\n",
      "0     394\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def get_latlon_corners(center_lat, center_lon, points, img_size=128, gsd=10):\n",
    "    latlons = []\n",
    "    for x, y in points:\n",
    "        dx = x - img_size / 2\n",
    "        dy = y - img_size / 2\n",
    "        dx_m = dx * gsd\n",
    "        dy_m = dy * gsd\n",
    "        dlat = dy_m / 111320\n",
    "        dlon = dx_m / (111320 * np.cos(np.deg2rad(center_lat)))\n",
    "        lat = center_lat + dlat\n",
    "        lon = center_lon + dlon\n",
    "        latlons.append((lon, lat))\n",
    "    return latlons\n",
    "\n",
    "def folder_to_gdf(folder, split):\n",
    "    data = []\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname.endswith('.txt'):\n",
    "            image_name = fname.replace('.txt', '.png')\n",
    "            lat_str, lon_str = image_name.replace('.png', '').split('_')\n",
    "            center_lat = float(lat_str)\n",
    "            center_lon = float(lon_str)\n",
    "            label_path = os.path.join(folder, fname)\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 9:\n",
    "                        continue\n",
    "                    class_id = int(parts[0])\n",
    "                    coords = list(map(float, parts[1:]))\n",
    "                    points = np.array(coords).reshape(4,2) * 128\n",
    "                    latlons = get_latlon_corners(center_lat, center_lon, points)\n",
    "                    poly = Polygon(latlons)\n",
    "                    data.append({\n",
    "                        'image_name': image_name,\n",
    "                        'class': class_id,\n",
    "                        'geometry': poly,\n",
    "                        'split': split\n",
    "                    })\n",
    "    return gpd.GeoDataFrame(data, crs=\"EPSG:4326\")\n",
    "\n",
    "# ----- Config -----\n",
    "folders = {\n",
    "    \"train\": \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/train/yolo_obb_labels\",\n",
    "    \"val\":   \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/val/yolo_obb_labels\",\n",
    "    \"test\":  \"/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/test/yolo_obb_labels\"\n",
    "}\n",
    "\n",
    "gdfs = [folder_to_gdf(folders[split], split) for split in [\"train\", \"val\", \"test\"]]\n",
    "gdf_all = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=\"EPSG:4326\")\n",
    "\n",
    "# ----- Buffer geometries (0.5 meter buffer) -----\n",
    "# Use auto UTM for metric buffering\n",
    "gdf_proj = gdf_all.to_crs(gdf_all.estimate_utm_crs())\n",
    "gdf_proj['buffered'] = gdf_proj.geometry.buffer(16.033)   # <-- 0.5 meters!\n",
    "\n",
    "# Split out for clean overlap analysis\n",
    "test_proj = gdf_proj[gdf_proj['split'] == 'test'].copy()\n",
    "trainval_proj = gdf_proj[gdf_proj['split'].isin(['train', 'val'])].copy()\n",
    "\n",
    "# Build spatial index for train+val\n",
    "tv_sindex = trainval_proj['buffered'].sindex\n",
    "\n",
    "def overlaps_trainval(test_geom):\n",
    "    candidates = list(tv_sindex.intersection(test_geom.bounds))\n",
    "    # Use .iloc for pandas fancy indexing\n",
    "    if not candidates:\n",
    "        return False\n",
    "    candidate_geoms = trainval_proj['buffered'].iloc[candidates]\n",
    "    return candidate_geoms.intersects(test_geom).any()\n",
    "\n",
    "test_proj['overlaps_trainval'] = test_proj['buffered'].apply(overlaps_trainval)\n",
    "\n",
    "# Filter: clean test set = test instances NOT overlapping train/val\n",
    "clean_test_proj = test_proj[~test_proj['overlaps_trainval']].copy()\n",
    "\n",
    "print(f\"Total test instances: {len(test_proj)}\")\n",
    "print(f\"Clean test instances (no overlap with train/val): {len(clean_test_proj)}\")\n",
    "\n",
    "# If you want the corresponding original rows (EPSG:4326)\n",
    "clean_test_indices = clean_test_proj.index\n",
    "clean_test_gdf = gdf_all.loc[clean_test_indices].copy()\n",
    "\n",
    "# Optional: Save to file\n",
    "clean_test_gdf.to_file(\"clean_test_set.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "# (Optional) View by class or image name:\n",
    "print(clean_test_gdf['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79b06423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- After you already have:\n",
    "# test_proj = original buffered test GeoDataFrame (before filtering)\n",
    "# unique_test_proj = only the unique kilns (after filtering)\n",
    "\n",
    "# Count original kilns per test image\n",
    "orig_counts = test_proj.groupby('image_name').size().rename('orig_count')\n",
    "# Count unique (retained) kilns per test image\n",
    "unique_counts = clean_test_proj.groupby('image_name').size().rename('unique_count')\n",
    "\n",
    "# Merge for easy stats\n",
    "stats_df = pd.concat([orig_counts, unique_counts], axis=1).fillna(0).astype(int)\n",
    "stats_df['removed'] = stats_df['orig_count'] - stats_df['unique_count']\n",
    "stats_df['affected'] = stats_df['removed'] > 0\n",
    "stats_df['all_removed'] = stats_df['unique_count'] == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d37e3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test label files (images): 15738\n",
      "Affected (lost ≥1 kiln):           5460\n",
      "Not affected (all labels kept):    10278\n",
      "Completely empty (all removed):    4393\n",
      "\n",
      "Sanity check table (with sum):\n",
      "   removed  num_files  total_kilns_removed\n",
      "0        0      10278                    0\n",
      "1        1       4593                 4593\n",
      "2        2        657                 1314\n",
      "3        3        138                  414\n",
      "4        4         38                  152\n",
      "5        5         14                   70\n",
      "6        6          9                   54\n",
      "7        7          7                   49\n",
      "8        8          1                    8\n",
      "9        9          2                   18\n",
      "10      10          1                   10\n",
      "11     SUM      15738                 6682\n",
      "\n",
      "Markdown Table:\n",
      "\n",
      "| removed   |   num_files |   total_kilns_removed |\n",
      "|:----------|------------:|----------------------:|\n",
      "| 0         |       10278 |                     0 |\n",
      "| 1         |        4593 |                  4593 |\n",
      "| 2         |         657 |                  1314 |\n",
      "| 3         |         138 |                   414 |\n",
      "| 4         |          38 |                   152 |\n",
      "| 5         |          14 |                    70 |\n",
      "| 6         |           9 |                    54 |\n",
      "| 7         |           7 |                    49 |\n",
      "| 8         |           1 |                     8 |\n",
      "| 9         |           2 |                    18 |\n",
      "| 10        |           1 |                    10 |\n",
      "| SUM       |       15738 |                  6682 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "n_total = len(stats_df)\n",
    "n_affected = stats_df['affected'].sum()\n",
    "n_unaffected = (~stats_df['affected']).sum()\n",
    "n_all_removed = stats_df['all_removed'].sum()\n",
    "\n",
    "print(f\"Total test label files (images): {n_total}\")\n",
    "print(f\"Affected (lost ≥1 kiln):           {n_affected}\")\n",
    "print(f\"Not affected (all labels kept):    {n_unaffected}\")\n",
    "print(f\"Completely empty (all removed):    {n_all_removed}\")\n",
    "\n",
    "# Histogram of # removed per file (all bins)\n",
    "removed_hist = stats_df['removed'].value_counts().sort_index()\n",
    "\n",
    "# Build crosscheck table: removed, num_files, total_kilns_removed\n",
    "crosscheck = []\n",
    "for removed, num_files in removed_hist.items():\n",
    "    total_kilns = removed * num_files\n",
    "    crosscheck.append([removed, num_files, total_kilns])\n",
    "\n",
    "crosscheck_table = pd.DataFrame(crosscheck, columns=['removed', 'num_files', 'total_kilns_removed'])\n",
    "\n",
    "# Add sum row at the bottom\n",
    "sum_row = pd.DataFrame([{\n",
    "    'removed': 'SUM',\n",
    "    'num_files': crosscheck_table['num_files'].sum(),\n",
    "    'total_kilns_removed': crosscheck_table['total_kilns_removed'].sum()\n",
    "}])\n",
    "crosscheck_table = pd.concat([crosscheck_table, sum_row], ignore_index=True)\n",
    "\n",
    "print(\"\\nSanity check table (with sum):\")\n",
    "print(crosscheck_table)\n",
    "print(\"\\nMarkdown Table:\\n\")\n",
    "print(crosscheck_table.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f02aa625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Summary Statistics\n",
      "| Statistic                                     |   Value |\n",
      "|:----------------------------------------------|--------:|\n",
      "| Total test images (label files)               |   15738 |\n",
      "| Test images now empty (0 unique kilns)        |    4393 |\n",
      "| Test images with ≥1 unique kiln               |   11345 |\n",
      "| Total kiln instances (before filtering)       |   21100 |\n",
      "| Total unique kiln instances (after filtering) |   14418 |\n",
      "| Total kilns removed (overlap)                 |    6682 |\n",
      "| Test images affected (lost ≥1 kiln)           |    5460 |\n",
      "\n",
      "### Sanity Check Table (with sum row)\n",
      "| removed   |   num_files |   total_kilns_removed |\n",
      "|:----------|------------:|----------------------:|\n",
      "| 0         |       10278 |                     0 |\n",
      "| 1         |        4593 |                  4593 |\n",
      "| 2         |         657 |                  1314 |\n",
      "| 3         |         138 |                   414 |\n",
      "| 4         |          38 |                   152 |\n",
      "| 5         |          14 |                    70 |\n",
      "| 6         |           9 |                    54 |\n",
      "| 7         |           7 |                    49 |\n",
      "| 8         |           1 |                     8 |\n",
      "| 9         |           2 |                    18 |\n",
      "| 10        |           1 |                    10 |\n",
      "| SUM       |       15738 |                  6682 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -- Crosscheck Table (as before) --\n",
    "removed_hist = stats_df['removed'].value_counts().sort_index()\n",
    "crosscheck = []\n",
    "for removed, num_files in removed_hist.items():\n",
    "    total_kilns = removed * num_files\n",
    "    crosscheck.append([removed, num_files, total_kilns])\n",
    "\n",
    "crosscheck_table = pd.DataFrame(crosscheck, columns=['removed', 'num_files', 'total_kilns_removed'])\n",
    "\n",
    "# Add sum row at the bottom\n",
    "sum_row = pd.DataFrame([{\n",
    "    'removed': 'SUM',\n",
    "    'num_files': crosscheck_table['num_files'].sum(),\n",
    "    'total_kilns_removed': crosscheck_table['total_kilns_removed'].sum()\n",
    "}])\n",
    "crosscheck_table = pd.concat([crosscheck_table, sum_row], ignore_index=True)\n",
    "\n",
    "# -- Summary Table --\n",
    "summary_table = pd.DataFrame([\n",
    "    [\"Total test images (label files)\",               len(stats_df)],\n",
    "    [\"Test images now empty (0 unique kilns)\",        (stats_df['unique_count'] == 0).sum()],\n",
    "    [\"Test images with ≥1 unique kiln\",               (stats_df['unique_count'] > 0).sum()],\n",
    "    [\"Total kiln instances (before filtering)\",       stats_df['orig_count'].sum()],\n",
    "    [\"Total unique kiln instances (after filtering)\", stats_df['unique_count'].sum()],\n",
    "    [\"Total kilns removed (overlap)\",                 stats_df['removed'].sum()],\n",
    "    [\"Test images affected (lost ≥1 kiln)\",           (stats_df['removed'] > 0).sum()]\n",
    "], columns=[\"Statistic\", \"Value\"])\n",
    "\n",
    "# -- Print both tables as markdown --\n",
    "print(\"\\n### Summary Statistics\")\n",
    "print(summary_table.to_markdown(index=False))\n",
    "print(\"\\n### Sanity Check Table (with sum row)\")\n",
    "print(crosscheck_table.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b763a",
   "metadata": {},
   "source": [
    "| removed   |   num_files |   total_kilns_removed |\n",
    "|:----------|------------:|----------------------:|\n",
    "| 0         |       10278 |                     0 |\n",
    "| 1         |        4593 |                  4593 |\n",
    "| 2         |         657 |                  1314 |\n",
    "| 3         |         138 |                   414 |\n",
    "| 4         |          38 |                   152 |\n",
    "| 5         |          14 |                    70 |\n",
    "| 6         |           9 |                    54 |\n",
    "| 7         |           7 |                    49 |\n",
    "| 8         |           1 |                     8 |\n",
    "| 9         |           2 |                    18 |\n",
    "| 10        |           1 |                    10 |\n",
    "| SUM       |       15738 |                  6682 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b2ed3",
   "metadata": {},
   "source": [
    "| removed   |   num_files |   total_kilns_removed |\n",
    "|:----------|------------:|----------------------:|\n",
    "| 0         |       10278 |                     0 |\n",
    "| 1         |        4593 |                  4593 |\n",
    "| 2         |         657 |                  1314 |\n",
    "| 3         |         138 |                   414 |\n",
    "| 4         |          38 |                   152 |\n",
    "| 5         |          14 |                    70 |\n",
    "| 6         |           9 |                    54 |\n",
    "| 7         |           7 |                    49 |\n",
    "| 8         |           1 |                     8 |\n",
    "| 9         |           2 |                    18 |\n",
    "| 10        |           1 |                    10 |\n",
    "| SUM       |       15738 |                  6682 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0ede6",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "| Statistic                                     |   Value |\n",
    "|:----------------------------------------------|--------:|\n",
    "| Total test images (label files)               |   15738 |\n",
    "| Test images now empty (0 unique kilns)        |    4393 |\n",
    "| Test images with ≥1 unique kiln               |   11345 |\n",
    "| Total kiln instances (before filtering)       |   21100 |\n",
    "| Total unique kiln instances (after filtering) |   14418 |\n",
    "| Total kilns removed (overlap)                 |    6682 |\n",
    "| Test images affected (lost ≥1 kiln)           |    5460 |\n",
    "\n",
    "### Sanity Check Table (with sum row)\n",
    "| removed   |   num_files |   total_kilns_removed |\n",
    "|:----------|------------:|----------------------:|\n",
    "| 0         |       10278 |                     0 |\n",
    "| 1         |        4593 |                  4593 |\n",
    "| 2         |         657 |                  1314 |\n",
    "| 3         |         138 |                   414 |\n",
    "| 4         |          38 |                   152 |\n",
    "| 5         |          14 |                    70 |\n",
    "| 6         |           9 |                    54 |\n",
    "| 7         |           7 |                    49 |\n",
    "| 8         |           1 |                     8 |\n",
    "| 9         |           2 |                    18 |\n",
    "| 10        |           1 |                    10 |\n",
    "| SUM       |       15738 |                  6682 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecbbbabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# These are your stats_df index (image_name) for files not affected\n",
    "unaffected_images = stats_df[stats_df['removed'] == 0].index.tolist()  # e.g. [\"23.4283_68.4472.png\", ...]\n",
    "unaffected_basenames = [os.path.splitext(img)[0] for img in unaffected_images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f95176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images_dir = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/test/images'\n",
    "test_labels_dir = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/test/yolo_aa_labels'\n",
    "# out_images_dir = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/stratified_split/clean_test/images'\n",
    "out_labels_dir = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/stratified_split/clean_test/yolo_aa_labels'\n",
    "\n",
    "# os.makedirs(out_images_dir, exist_ok=True)\n",
    "os.makedirs(out_labels_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31332a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for base in unaffected_basenames:\n",
    "    img_file = base + '.png'\n",
    "    label_file = base + '.txt'\n",
    "    # src_img = os.path.join(test_images_dir, img_file)\n",
    "    src_lbl = os.path.join(test_labels_dir, label_file)\n",
    "    # dst_img = os.path.join(out_images_dir, img_file)\n",
    "    dst_lbl = os.path.join(out_labels_dir, label_file)\n",
    "    # if os.path.exists(src_img):\n",
    "    #     shutil.copy2(src_img, dst_img)\n",
    "    if os.path.exists(src_lbl):\n",
    "        shutil.copy2(src_lbl, dst_lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a94215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "unaffected_images = stats_df[stats_df['removed'] == 0].index.tolist()\n",
    "unaffected_basenames = [os.path.splitext(img)[0] for img in unaffected_images]\n",
    "\n",
    "test_labels_dir = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/final_data_neurips_2025/test/yolo_aa_labels'\n",
    "out_labels_dir_1 = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/filtered_test_unaffected/yolo_obb_labels'\n",
    "out_labels_dir_2 = '/home/rishabh.mondal/Brick-Kilns-project/ijcai_2025_kilns/data/processed_data/sentinel/filtered_test_unaffected/yolo_aa_labels'\n",
    "\n",
    "os.makedirs(out_labels_dir_1, exist_ok=True)\n",
    "os.makedirs(out_labels_dir_2, exist_ok=True)\n",
    "\n",
    "for base in unaffected_basenames:\n",
    "    label_file = base + '.txt'\n",
    "    src_lbl = os.path.join(test_labels_dir, label_file)\n",
    "    dst_lbl1 = os.path.join(out_labels_dir_1, label_file)\n",
    "    dst_lbl2 = os.path.join(out_labels_dir_2, label_file)\n",
    "    if os.path.exists(src_lbl):\n",
    "        shutil.copy2(src_lbl, dst_lbl1)\n",
    "        shutil.copy2(src_lbl, dst_lbl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c16d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train kilns: 55493\n",
      "Unique val kilns:   21042\n",
      "Unique test kilns:  21100\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Project to metric CRS for buffering\n",
    "gdf_proj = gdf_all.to_crs(gdf_all.estimate_utm_crs())\n",
    "gdf_proj['buffered'] = gdf_proj.geometry.buffer((16.033))   # 0.5 meter buffer\n",
    "\n",
    "\n",
    "def count_unique_kilns(gdf_proj, split):\n",
    "    gdf_split = gdf_proj[gdf_proj['split'] == split].copy()\n",
    "    # Use spatial join to find overlapping polygons (kilns)\n",
    "    sindex = gdf_split['buffered'].sindex\n",
    "    visited = set()\n",
    "    groups = []\n",
    "    for idx, geom in gdf_split['buffered'].items():\n",
    "        if idx in visited:\n",
    "            continue\n",
    "        # Find all others that touch this one\n",
    "        matches = set(sindex.intersection(geom.bounds))\n",
    "        group = {idx}\n",
    "        for other in matches:\n",
    "            if other != idx:\n",
    "                other_geom = gdf_split['buffered'].iloc[other]\n",
    "                if geom.intersects(other_geom):\n",
    "                    group.add(other)\n",
    "        groups.append(group)\n",
    "        visited.update(group)\n",
    "    # Each group represents a unique kiln\n",
    "    return len(groups)\n",
    "\n",
    "n_unique_train = count_unique_kilns(gdf_proj, 'train')\n",
    "n_unique_val   = count_unique_kilns(gdf_proj, 'val')\n",
    "n_unique_test  = count_unique_kilns(gdf_proj, 'test')\n",
    "\n",
    "print(f\"Unique train kilns: {n_unique_train}\")\n",
    "print(f\"Unique val kilns:   {n_unique_val}\")\n",
    "print(f\"Unique test kilns:  {n_unique_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a0db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rishabh_sat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
